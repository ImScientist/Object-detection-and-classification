{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/A.IVA/Documents/jupyter_notebooks/coursera_and_blogs/clouds'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### 1.1 Example from MaskRCNN: Add a different backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load a pre-trained model for classification and return only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "\n",
    "# MaskRCNN needs to know the number of output channels in a backbone. For mobilenet_v2, it's 1280 so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                     output_size=14,\n",
    "                                                     sampling_ratio=2)\n",
    "\n",
    "model = MaskRCNN(backbone,\n",
    "                 num_classes=2,\n",
    "                 rpn_anchor_generator=anchor_generator,\n",
    "                 box_roi_pool=roi_pooler,\n",
    "                 mask_roi_pool=mask_roi_pooler)\n",
    "model.eval()\n",
    "\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### 1.2 Finetuning a pretrained model (what we have been doing until now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has num_classes which is user-defined\n",
    "# get number of input features for the classifier\n",
    "# replace the pre-trained head with a new one\n",
    "num_classes = 2\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### 1.3 Repeat 1.1 but with a model loaded from `pretrainedmodels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Look at [this repo](https://github.com/Cadene/pretrained-models.pytorch#modelfeatures) for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fbresnet152', 'bninception', 'resnext101_32x4d', 'resnext101_64x4d', 'inceptionv4', 'inceptionresnetv2', 'alexnet', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'inceptionv3', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19', 'nasnetamobile', 'nasnetalarge', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn131', 'dpn107', 'xception', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'cafferesnet101', 'pnasnet5large', 'polynet']\n"
     ]
    }
   ],
   "source": [
    "print(pretrainedmodels.model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "architecture = 'resnet18'\n",
    "model = pretrainedmodels.__dict__[architecture](num_classes=1000, pretrained='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 14, 14])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone = model.features\n",
    "\n",
    "tmp = torch.as_tensor(np.zeros(shape=(1,3,224*2,224*2)), dtype=torch.float32)\n",
    "backbone(tmp).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyBackbone(object):\n",
    "\n",
    "    def __init__(self, backbone, out_channels):\n",
    "        self.backbone = backbone\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        return self.backbone(input)\n",
    "\n",
    "\n",
    "my_backbone = MyBackbone(backbone, 512)\n",
    "hasattr(my_backbone, \"out_channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import MaskRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                     output_size=14,\n",
    "                                                     sampling_ratio=2)\n",
    "\n",
    "model = MaskRCNN(my_backbone,\n",
    "                 num_classes=2,\n",
    "                 rpn_anchor_generator=anchor_generator,\n",
    "                 box_roi_pool=roi_pooler,\n",
    "                 mask_roi_pool=mask_roi_pooler)\n",
    "model.eval()\n",
    "\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " dict,\n",
       " dict_keys(['boxes', 'labels', 'scores', 'masks']),\n",
       " torch.Size([100, 4]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions), type(predictions[0]), predictions[0].keys(), predictions[0]['boxes'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
